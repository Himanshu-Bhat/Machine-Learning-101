{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<font color='#1d63e1'>Gathering Data</font>**\n",
    "\n",
    "#### <font color='#f0f375'>Data can be gathered from following of the  Data Sources</font>\n",
    "\n",
    "1. *<font color='#ea9b2d'> By CSV </font>*\n",
    "2. *<font color='#ea9b2d'> Json/SQL </font>*\n",
    "3. *<font color='#ea9b2d'> fetch API </font>*\n",
    "4. *<font color='#ea9b2d'> Web Scrapping </font>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font color='#2dea71'> CSV </font>**\n",
    "####  **<font color='#734fd0'> # Loading From Local Directory </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Odometer (KM)</th>\n",
       "      <th>Doors</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>35431.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15323.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW</td>\n",
       "      <td>Blue</td>\n",
       "      <td>192714.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Honda</td>\n",
       "      <td>White</td>\n",
       "      <td>84714.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>White</td>\n",
       "      <td>154365.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nissan</td>\n",
       "      <td>Blue</td>\n",
       "      <td>181577.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14043.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make Colour  Odometer (KM)  Doors    Price\n",
       "0   Honda  White        35431.0    4.0  15323.0\n",
       "1     BMW   Blue       192714.0    5.0  19943.0\n",
       "2   Honda  White        84714.0    4.0  28343.0\n",
       "3  Toyota  White       154365.0    4.0  13434.0\n",
       "4  Nissan   Blue       181577.0    3.0  14043.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Loading file from directory \n",
    "df = pd.read_csv(r'E:\\DataSet\\Machine Learning 101/Sample1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='#7edf4d'> For more info on pandas refer : [ https://pandas.pydata.org/docs/reference/io.html ] </font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<font color='#734fd0'> # Loading From Server </font>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "url = 'URL of CSV file'\n",
    "header = {'host':'Host_name','key':'API_Key'}\n",
    "\n",
    "req = requests.get(url,header)                # header may or may not be required\n",
    "data = StringIO(req.text)\n",
    "\n",
    "df = pd.read_csv(data).head()\n",
    "\n",
    "Or just use df = pd.read_csv('url_link')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**<font color='#7edf4d'> Different pd_csv Parameter </font>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# sep Parameter \n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',sep=',')\n",
    "\n",
    "# Setting columns names if not exists\n",
    "columns = ['Col1','Col2','Col3'.'Col4','Col5']\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',name=columns)\n",
    "\n",
    "# Making column as Index Column\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',index_col='Make')\n",
    "\n",
    "# Ignoring Index\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',ignore_index=True)\n",
    " \n",
    "# Making 1st row as Headings/Columns_Name (if Default Heading is corupt)\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',header=1)\n",
    "\n",
    "# Ignoring Header/Columns Name\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',header=None)\n",
    "\n",
    "# Selecting Specific Columns to be in DataFrame\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',usecols=['Make','Colur','Price'])\n",
    "\n",
    "# Skipping Rows\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',skiprows=[1,2,3])\n",
    "\n",
    "# Specifing number of rows to be included\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',nrows=1000)\n",
    "\n",
    "# Skipping Bad Lines that can cause Error while loading Data\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',error_bad_lines=False)\n",
    "\n",
    "# Specifing Column Dtype while loading the DataFrame \n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',dtypes={'Doors':int})\n",
    "\n",
    "# Parsing Dates converted to strings as Dates Dtype\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',parse_dates=['Column_Name_Having_Date'])\n",
    "\n",
    "# Renaming Column Values / Transforming Column Values \n",
    "  def renameX(column_value):\n",
    "      if column_value = 'Honda'\n",
    "          return 'Hero Honda'\n",
    "      else \n",
    "          return column_value\n",
    "          \n",
    "  df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',convertors={'Make':renameX})\n",
    "          \n",
    "# Specifing what to consider as NaN Values\n",
    "df = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',na_values=['-'])     # Now '-' will be considered as NaN\n",
    "\n",
    "\n",
    "# Loading Huge DataSet into Chunks\n",
    "dff = pd.read_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv',chunksize=1000)\n",
    "\n",
    "    for chunks in dff:\n",
    "        operations_to_perform(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **<font color='#2dea71'> Json / SQL </font>**\n",
    "####  **<font color='#734fd0'> # For Json (Loading From Local Directory)  </font>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample3.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **<font color='#734fd0'> # For Json (Loading from server)  </font>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **<font color='#734fd0'> # For SQL </font>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install mysql.connector                                          # if not already installed\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "conn = mysql.connector.connect(host='localhost',user='root',password='',database='DataBase_Name'\n",
    "\n",
    "df = pd.read_sql_query('SELECT * FROM Table_Name',conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font color='#2dea71'> API </font>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# RapidAPI & few others provide free API\n",
    "# Note: API fetching requires API Key\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url_X    = 'https:site_address.com{API_KEY}'\n",
    "header_X = {'host':'host_Address','key':'host_key'}                # May or May not be required to pass to requests\n",
    "                                                                         \n",
    "req = requests.get(url=url_X,headers=header_X)                     # if headers is not present then, request is considered as                                                                      Bot Attempt to access the API, May/May not be required.\n",
    "req.status_code  # return status after hitting API\n",
    "df = pd.DataFrame(req.json()['Dictionary_Key'])\n",
    "\n",
    "Or If API Data is storeed with in different pages, Then try \n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in range(1,X):                                                   # X--> Total number of pages\n",
    "    req = requests.get('https:urls{API Key}')\n",
    "    temp_df = pd.DataFrame(req.json()['dictionary_key_having_values'])\n",
    "    df = pd.append(temp_df,ignore_index=True)\n",
    "    \n",
    "# Saving Dataset to CSV\n",
    "pd.to_csv(r'C:\\Users\\Legion\\Desktop\\100 Days of ML\\Sample Data\\Sample1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>domains</th>\n",
       "      <th>web_pages</th>\n",
       "      <th>alpha_two_code</th>\n",
       "      <th>name</th>\n",
       "      <th>state-province</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>[marywood.edu]</td>\n",
       "      <td>[http://www.marywood.edu]</td>\n",
       "      <td>US</td>\n",
       "      <td>Marywood University</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>[lindenwood.edu]</td>\n",
       "      <td>[http://www.lindenwood.edu/]</td>\n",
       "      <td>US</td>\n",
       "      <td>Lindenwood University</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>[sullivan.edu]</td>\n",
       "      <td>[https://sullivan.edu/]</td>\n",
       "      <td>US</td>\n",
       "      <td>Sullivan University</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         country           domains                     web_pages  \\\n",
       "0  United States    [marywood.edu]     [http://www.marywood.edu]   \n",
       "1  United States  [lindenwood.edu]  [http://www.lindenwood.edu/]   \n",
       "2  United States    [sullivan.edu]       [https://sullivan.edu/]   \n",
       "\n",
       "  alpha_two_code                   name state-province  \n",
       "0             US    Marywood University           None  \n",
       "1             US  Lindenwood University           None  \n",
       "2             US    Sullivan University           None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test API Call\n",
    "import requests \n",
    "import pandas as pd\n",
    "url = 'http://universities.hipolabs.com/search?country=United+States'\n",
    "req = requests.get(url)\n",
    "print(req.status_code)\n",
    "\n",
    "df = pd.DataFrame(req.json())\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## **<font color='#2dea71'> Web Scrapping </font>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header_X = {'User-Agent':'Specifing_Host/Agent_is_Not_Bot'}\n",
    "url_X    = 'https:site_address.com'\n",
    "\n",
    "webpage_X = requests.get(url=url_X,headers=header_X).text             # returns webpages View_Source text code\n",
    "\n",
    "soup = BeautifulSoup(webpage_X,'lxml')                                # to ease the HTML Parsing\n",
    "\n",
    "print(soup.pretify())                                                 # prints how HTML Page is organised\n",
    "\n",
    "# Depending on requirement, \n",
    "soup.find_all('h1').text                                              # returns all H1 tag from website\n",
    "soup.find('h1').text                                                  # returns single H2 tag from website\n",
    "\n",
    "# Depending on requirement\n",
    "for i in soup.find_all('h1'):\n",
    "    print(i.text.strip())\n",
    "    \n",
    "# Depending on requirement\n",
    "for i in soup.find_all('p',class='Specific_class'):                    # Here, 'p' is a;so a tag having specific class\n",
    "    print(i.text.strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Web Scrapping for Company-Review-Portal\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header_X = {'User-Agent':'Specifing_Host/Agent_is_Not_Bot'}\n",
    "url_X    = 'https:site_address.com'\n",
    "\n",
    "webpage_X = requests.get(url=url_X,headers=header_X).text   \n",
    "\n",
    "soup = BeautifulSoup(webpage_X,'lxml')   # to ease the HTML Parsing\n",
    "\n",
    "company = soup.find_all('div',class_='company_content_wrapper').text\n",
    "\n",
    "name =   []\n",
    "rating = []\n",
    "average_salary = []\n",
    "\n",
    "for i in company:\n",
    "        name.append(i.find('H2',class_='ABC')\n",
    "        rating.append(i.find('p',class_='XYZ')\n",
    "        average_salary(i.find('p',class_='RST')\n",
    "        \n",
    "df = DataFrame({'Name':name,'Rating':rating,'Average Salary':average_salary})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
